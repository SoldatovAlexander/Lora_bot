# =========================
# Настройки проекта (пример)
# =========================

# Базовая 4-битная модель (как в лекции/практике)
BASE_MODEL_NAME=unsloth/llama-3-8b-Instruct-bnb-4bit

# Каталог с ПРЕДОБУЧЕННЫМИ QLoRA/LoRA-адаптерами и (опционально) токенайзером.
# Внутри должны быть минимум:
#   - adapter_config.json
#   - adapter_model.safetensors (или adapter_model.bin)
# И часто также:
#   - tokenizer.json / tokenizer_config.json / special_tokens_map.json / chat_template.jinja
ADAPTER_DIR=./model/adapters

# (Необязательно) Если адаптеры лежат в репозитории HF, можно скачать их скриптом:
# ADAPTER_HF_REPO=your-org/your-adapter-repo
# ADAPTER_HF_REVISION=main

# Папка для кэша моделей HF (полезно на сервере, чтобы держать кэш в одном месте)
HF_HOME=./.cache/huggingface

# Параметры генерации по умолчанию
DEFAULT_MAX_NEW_TOKENS=180
DEFAULT_TEMPERATURE=0.7

# Логирование (по заданию базовый уровень WARNING)
LOG_LEVEL=WARNING
